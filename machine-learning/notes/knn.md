# ğŸŒŸ KNN - K Nearest Neighbors ğŸŒŸ

KNN is one of the simplest **Supervised ML algorithms**, commonly used for:

- ğŸ· **Classification** (It classifies a data point based on how its neighbors are classified)
- ğŸ’¾ It stores all available cases and classifies new ones based on a **similarity measure**
- ğŸ”¢ **k** in KNN refers to the number of nearest neighbors considered in the majority voting process

## KNN Algorithm ğŸ§ 
KNN is based on **feature similarity**. Choosing the right value of **k** is called **parameter tuning** and is key for better accuracy. ğŸ¯

### How to choose a value for **K**:
- ğŸ§® **Sqrt(n)**, where **n** is the total number of data points
- ğŸ”€ Choose an **odd** value of **K** to avoid confusion between two classes of data

### When to use KNN? ğŸ¤”
- ğŸ· **Data is labeled**
- ğŸ”‡ **Data is noise-free**
- ğŸ“Š **Dataset is small**

**Note:** KNN is a lazy learner. ğŸ’¤ It doesn't learn a function from the training set but instead memorizes the data!

## How does the KNN algorithm work? ğŸ› 
- We use the **Euclidean distance** to calculate the nearest neighbors ğŸ‘¥.
- According to the **Euclidean distance formula**, the distance between two points in the plane with coordinates (x, y) and (a, b) is:
dist(d) = sqrt{(qâ‚ - pâ‚)Â² + (qâ‚‚ - pâ‚‚)Â²}

## Recap ğŸ“
1. ğŸ”¢ A positive integer **k** is specified, along with a new sample.
2. ğŸ” We select the **k entries** in our database that are closest to the new sample.
3. ğŸ† We find the most common classification among these entries.
4. ğŸ“ This classification is assigned to the new sample!

